import scrapy
from scrapy.http.response.html import HtmlResponse
from urllib.parse import urljoin
import pandas as pd
from datetime import datetime
import re

class SuumoSpider(scrapy.Spider):
    name = "suumo"  # スパイダーの名前を定義

    origin = "https://suumo.jp"
    allowed_domains = ["suumo.jp"]
    start_urls = [
        "https://suumo.jp/jj/chintai/ichiran/FR301FC005/?ar=030&bs=040&ta=13&sc=13113&cb=0.0&ct=8.0&mb=0&mt=9999999&et=20&cn=5&shkr1=03&shkr2=03&shkr3=03&shkr4=03&sngz=&po1=25&po2=99&pc=50"
    ]
    
    properties = []  # 物件情報を保存するためのリストです

    def parse(self, response: HtmlResponse):
        """
        スパイダーが最初に呼ばれるメソッドです。
        各ページの物件データを処理し、次のページがあればリクエストを送ります。
        """
        # 現在のページの物件データを処理します
        self.handle_response(response)

        # 次のページがあるかを確認します
        next_page = response.css("a.pagination-next::attr(href)").get()
        if next_page:
            # 次のページがあれば、そのページをリクエストします
            yield scrapy.Request(url=urljoin(self.origin, next_page), callback=self.parse)

    def handle_response(self, response: HtmlResponse):
        """
        物件データを処理するためのメソッドです。
        各物件の情報を抽出してリストに保存します。
        """
        # 物件情報を含む各要素に対して処理を行うループ
        for row in response.css("li"):
            # タイトルを取得
            title = row.css("div.cassetteitem_content-title::text").get(default='').strip()
            
            # 家賃情報を取得
            rent_price = row.css("div.detailbox-property-point::text").get(default='').strip()
            
            # 物件の詳細情報を取得
            property_table = row.css("div.detailbox > div.detailbox-property > table")
            plan_of_house = property_table.css("td.detailbox-property--col3 > div:nth-child(1)::text").get(default='').strip()
            exclusive_area = property_table.css("td.detailbox-property--col3 > div:nth-child(2)::text").get(default='').strip()
            
            # 詳細な説明を取得
            detail_texts = row.css("div.detailbox-note div.detailnote-box div::text").getall()
            detail_texts = [t.strip() for t in detail_texts if t.strip() != ""]
            detail = "\n".join(detail_texts).strip()

            # HTMLコンテンツ全体を取得
            html_content = row.get()

            # 住所を正規表現で抽出
            address_match = re.search(r'<td class="detailbox-property-col">.*?<!-- 住所 -->(.*?)</td>', html_content, re.DOTALL)
            address = address_match.group(1).strip() if address_match else '住所不明'

            # デバッグ用に抽出した住所を表示
            print(f"住所: {address}")

            # 物件情報をリストに追加
            self.properties.append({
                "Title": title,
                "Rent Price": rent_price,
                "Plan of House": plan_of_house,
                "Exclusive Area": exclusive_area,
                "Detail": detail,
                "Address": address,
            })

    def close(self, reason):
        """
        スパイダーの全ての処理が終了した時に呼ばれるメソッドです。
        データを Excel ファイルとして保存します。
        """
        self.save_to_excel()

    def save_to_excel(self):
        """
        スクレイピングしたデータを Excel ファイルとして保存します。
        """
        # pandas を使ってデータをデータフレームに変換します
        df = pd.DataFrame(self.properties)

        # 現在の日付と時間をファイル名に組み込みます
        current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        file_name = f"suumo_data_{current_time}.xlsx"

        # Excel ファイルをダウンロードフォルダに保存します
        file_path = f"C:/Users/yukik/Downloads/{file_name}"
        df.to_excel(file_path, index=False)

        # ファイル保存が成功したことをログに出力します 
        self.log(f"Excelファイルが保存されました: {file_path}")
